When creating the Prom tool

update the prometheus.yml in the project directory (inside service discovery) with the content of the prometheus.yml in this current directory .

create an ( alertmanager.yml ) file in the root of project directory 

update it with the content in the alertmanager.yml in this current directory that you are 

/etc/prometheus/
    ‚îú‚îÄ‚îÄ prometheus.yml
    ‚îú‚îÄ‚îÄ alert-rules.yaml

-------------------------------------------------------------------------------------

Reference the rules in prometheus.yml

prometheus.yml is the main configuration file for Prometheus.
Inside it, you need a rule_files section that tells Prometheus where to find your alert rules.

----------------------------------------------------------------------------------
global:
  scrape_interval: 15s   # how often Prometheus scrapes targets
  evaluation_interval: 15s  # how often rules are evaluated

scrape_configs:
  - job_name: "myapp"
    static_configs:
      - targets: ["localhost:8080"]

rule_files:
  - "alert-rules.yaml"

------------------------------------------------------------------------------------

If you already have your prometheus configured and updated it, 

Now restart Prometheus 
----------------------------------------------------------------------------------
After updating configs:

systemctl restart prometheus


or, if running via Docker:

docker restart prometheus
--------------------------------------------------------------------------------------

rule_files is a sibling to scrape_configs, not inside it.

The path "alert-rules.yaml" can be:

A relative path (if the file is in the same folder as prometheus.yml).

Or an absolute path (e.g., /etc/prometheus/alert-rules.yaml).

Example with absolute path:

rule_files:
  - "/etc/prometheus/alert-rules.yaml"


You can also include multiple files:

rule_files:
  - "infra-rules.yaml"
  - "app-rules.yaml"

-------------------------------------------------------------------------------------
üîπ How this works

rule_files ‚Üí Prometheus looks inside alert-rules.yaml for alerting rules.

Every 15s (based on evaluation_interval), Prometheus checks if any rule condition is met.

If a condition is true for the defined for: time (e.g., 5m), Prometheus fires the alert.

Prometheus then sends that alert to Alertmanager (if configured).

---------------------------------------------------------------------------------------

üîπ 1. Install Alertmanager

If you already have Prometheus, Alertmanager usually comes alongside.
You can check with:

alertmanager --version


If not installed, grab it from Prometheus releases
.

üîπ 2. Configure Prometheus to use Alertmanager

In your prometheus.yml, add the alerting section:

global:
  scrape_interval: 15s
  external_labels:
    monitor: 'prometheus'

alerting:
  alertmanagers:
    - static_configs:
        - targets: ["localhost:9093"]   # default Alertmanager port

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

rule_files:
  - "alert-rules.yaml"


This tells Prometheus where to send alerts (to Alertmanager at localhost:9093).

üîπ 3. Create alertmanager.yml

This is the main config for Alertmanager. Place it in the same folder where Alertmanager runs.

Here‚Äôs a starter config with Slack + email but the comprehensive file is in the (prom-alertmanager.yml):

---------------------------------------------------------------------------
global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.gmail.com:587'   # for email
  smtp_from: 'your-email@gmail.com'
  smtp_auth_username: 'your-email@gmail.com'
  smtp_auth_password: 'your-app-password'   # use an app password, not your real Gmail password

route:
  receiver: 'default-receiver'
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 3h

receivers:
  - name: 'default-receiver'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXXXX/XXXXX/XXXXXXXX'
        channel: '#alerts'
    email_configs:
      - to: 'team-alerts@company.com'

---------------------------------------------------------------------------

üîπ 4. Start Alertmanager

Run:

alertmanager --config.file=alertmanager.yml


It should start on port 9093.

üîπ 5. Verify

Go to Prometheus UI ‚Üí http://localhost:9090/alerts ‚Üí You‚Äôll see your alerts.

Go to Alertmanager UI ‚Üí http://localhost:9093 ‚Üí You‚Äôll see active alerts being routed to Slack/email.

‚úÖ With this setup:

Prometheus evaluates rules from alert-rules.yaml.

Alerts get sent to Alertmanager (localhost:9093).

Alertmanager sends notifications to Slack + email.

----------------------------------------------------------------------------

To confirm if the rules we configured for the alerts are working   

1. Check Prometheus Web UI

Open the Prometheus UI in your browser:
üëâ http://<prometheus-server>:9090

Go to Status ‚Üí Rules

You should see all the alerts you defined in alert-rules.yaml.

Each rule will show its state: INACTIVE, PENDING, or FIRING.

Go to Alerts tab

This shows active alerts.

If a condition is true (e.g., high CPU), you‚Äôll see it as FIRING here.

üîπ 2. Check Prometheus Logs

If the rules file has errors, Prometheus will not load it and you‚Äôll see logs like:

level=error msg="Error loading rule file" file="alert-rules.yaml" err="yaml: line 10: did not find expected key"


So always check logs if rules don‚Äôt appear.

üîπ 3. Test a Rule

You can test expressions in the Prometheus ‚ÄúGraph‚Äù tab:

Copy the expr: from one of your alert rules.

Paste it into the query bar.

Run it ‚Äî you should see time series data.

If it evaluates to true for the for: duration (e.g., 5m), the alert will fire.

‚úÖ If you see your rules under Status ‚Üí Rules, you‚Äôre good ‚Äî Prometheus is evaluating them.
‚úÖ If you see alerts under Alerts tab, they‚Äôre firing and being sent to Alertmanager.

----------------------------------------------------------------------------------

Steps to running Alerts Manager

Steps to Run the Demo

Place your files:

/etc/prometheus/prometheus.yml
/etc/prometheus/prometheus-alert-rules.yml
/etc/alertmanager/prom-alertmanager.yml


Start Alertmanager:

alertmanager --config.file=/etc/alertmanager/prom-alertmanager.yml


Start Prometheus:

prometheus --config.file=/etc/prometheus/prometheus.yml


Verify in Prometheus UI (http://localhost:9090) ‚Üí Status ‚Üí Rules ‚Üí you should see your alert rules loaded.

Verify in Alertmanager UI (http://localhost:9093) ‚Üí active alerts should appear once rules fire.

---------------------------------------------------------------------------------


Optional: Simulate Alerts

Critical: stop a node exporter ‚Üí triggers NodeDown.

Warning: run CPU stress ‚Üí triggers HighCPUUsageWarning.

Info: send many 4xx requests ‚Üí triggers High4xxRate.

This way, you can confirm end-to-end alert routing to Slack, PagerDuty, and email.

-------------------------------------------------------------------------------------


Here‚Äôs a clear workflow diagram showing how your Prometheus + Alertmanager setup routes alerts based on severity.



+-------------------+
|  Prometheus Server|
|------------------|
| - Scrapes metrics |
|   from nodes,     |
|   apps, k8s       |
| - Evaluates rules |
|   in prometheus-  |
|   alert-rules.yml |
+-------------------+
          |
          |  Alerts fire
          v
+-------------------+
|   Alertmanager    |
|------------------|
| - prom-alert-     |
|   manager.yml     |
| - Routes alerts   |
|   based on labels |
|   (severity)      |
| - Groups alerts   |
|   to reduce noise |
+-------------------+
   |        |        |
   |        |        |
   v        v        v
+--------+  +--------+  +--------+
| Pager- |  | Slack  |  | Email  |
| Duty   |  | Channel|  | Team   |
| Critical| | Warning|  | Info   |
+--------+  +--------+  +--------+

---------------------------------------------------------------------

How the flow works

Prometheus scrapes metrics from your infrastructure, Kubernetes, and applications.

Rules from prometheus-alert-rules.yml are evaluated at every evaluation_interval (20s in your config).

Alerts fire if conditions are met (CPU > 95%, NodeDown, High 5xx rate, etc.).

Alertmanager receives the alerts and looks at severity labels:

critical ‚Üí PagerDuty (on-call escalation)

warning ‚Üí Slack channel (#infra-warnings)

info ‚Üí Email team

Grouping and deduplication reduces notification noise (alerts with same alertname + severity are grouped).

Teams take action based on severity: immediate (critical), monitor & plan (warning), informational (info).

------------------------------------------------------------------------

Dummy metric simulation script

Here‚Äôs a simple Bash script that will simulate metrics to trigger different severities without touching real infrastructure:

#!/bin/bash
# Simulate metric increments to fire alerts

# 1Ô∏è‚É£ CPU simulation (HighCPUUsageWarning)
curl -X POST http://localhost:9100/metrics/job/cpu_simulator -d "node_cpu_seconds_total{mode=\"idle\",instance=\"test-node\"} 10"

# 2Ô∏è‚É£ Memory simulation (HighMemoryUsageWarning)
curl -X POST http://localhost:9100/metrics/job/mem_simulator -d "node_memory_MemAvailable_bytes{instance=\"test-node\"} 1000000"

# 3Ô∏è‚É£ 5xx Error simulation (HighErrorRateCritical)
curl -X POST http://localhost:8080/metrics/job/http_simulator -d "http_requests_total{status=\"500\",instance=\"test-app\"} 100"


‚úÖ Notes:

Prometheus scrapes these endpoints (you can use a small mock app or node exporter).

Adjust values so they cross the thresholds defined in your rules.

Once alerts fire, check Alertmanager UI ‚Üí they will route to Slack, PagerDuty, or Email depending on severity.

----------------------------------------------------------------------------------
Original Script Purpose

This Bash script is a mock testing tool for Prometheus alerts. It simulates metrics that will cross alert thresholds, allowing you to test your alerting pipeline (Prometheus ‚Üí Alertmanager ‚Üí Slack/PagerDuty/Email) without affecting real infrastructure.

üîπ Step-by-step explanation
#!/bin/bash
# This line tells the system to use Bash to run this script

# Simulate metric increments to fire alerts

1Ô∏è‚É£ CPU Usage Simulation
curl -X POST http://localhost:9100/metrics/job/cpu_simulator \
     -d "node_cpu_seconds_total{mode=\"idle\",instance=\"test-node\"} 10"


This is meant to simulate high CPU usage.

node_cpu_seconds_total{mode="idle", instance="test-node"} is a metric Prometheus normally scrapes.

By posting a small value to a mock endpoint, the alert rule HighCPUUsageWarning thinks the CPU is busy.

curl -X POST sends data to a mock Prometheus exporter.

2Ô∏è‚É£ Memory Usage Simulation
curl -X POST http://localhost:9100/metrics/job/mem_simulator \
     -d "node_memory_MemAvailable_bytes{instance=\"test-node\"} 1000000"


Simulates low available memory to trigger HighMemoryUsageWarning.

Prometheus sees MemAvailable as very low ‚Üí calculates memory usage > 80% ‚Üí alert fires.

3Ô∏è‚É£ 5xx Error Rate Simulation
curl -X POST http://localhost:8080/metrics/job/http_simulator \
     -d "http_requests_total{status=\"500\",instance=\"test-app\"} 100"


Simulates HTTP 5xx errors to trigger HighErrorRateCritical.

Prometheus calculates error rate based on the metric ‚Üí if it crosses the threshold (>20%), it fires a critical alert.

üîπ Why this script is useful

You don‚Äôt need to crash your servers or apps to test alerts.

You can validate the full alerting pipeline:

Prometheus ‚Üí Alertmanager ‚Üí Slack/PagerDuty/Email.

You can test multiple severities in one go (critical, warning, info).

üîπ Better Version of the Script

Here‚Äôs a cleaner, more realistic version, using comments and functions for clarity:

#!/bin/bash
# Simulate metrics to trigger Prometheus alerts for testing

# Function to simulate CPU usage
simulate_cpu() {
  echo "[INFO] Simulating high CPU usage..."
  curl -X POST http://localhost:9100/metrics/job/cpu_simulator \
       -d "node_cpu_seconds_total{mode=\"idle\",instance=\"test-node\"} 10"
}

# Function to simulate low memory
simulate_memory() {
  echo "[INFO] Simulating low memory..."
  curl -X POST http://localhost:9100/metrics/job/mem_simulator \
       -d "node_memory_MemAvailable_bytes{instance=\"test-node\"} 1000000"
}

# Function to simulate 5xx errors
simulate_5xx() {
  echo "[INFO] Simulating 5xx HTTP errors..."
  curl -X POST http://localhost:8080/metrics/job/http_simulator \
       -d "http_requests_total{status=\"500\",instance=\"test-app\"} 100"
}

# Run all simulations
simulate_cpu
simulate_memory
simulate_5xx

echo "[DONE] Metrics simulated. Check Prometheus & Alertmanager."


‚úÖ Benefits of this version:

Clear functions for each alert type.

Logging messages for clarity.

Easier to extend (add more simulated metrics).

Safe and repeatable for testing.

If you want, I can now make a fully working mock metric exporter in Python that actually serves metrics for Prometheus to scrape ‚Äî this is much closer to real testing than just curl -X POST.

Do you want me to do that next?

You said:
yes
ChatGPT said:

Perfect ‚úÖ ‚Äî let‚Äôs create a simple Python mock metrics exporter that Prometheus can scrape. This is much safer and realistic than POSTing fake metrics, and it will let you simulate different alert severities for CPU, memory, and HTTP errors.

1Ô∏è‚É£ Python Mock Metrics Exporter (mock_exporter.py)
from flask import Flask, Response
import random
import time

app = Flask(__name__)

# Simulated metrics
cpu_usage = 50          # starts at 50%
memory_available = 8000 # in MB
http_5xx_count = 0
queue_jobs_pending = 20

@app.route("/metrics")
def metrics():
    global cpu_usage, memory_available, http_5xx_count, queue_jobs_pending

    # Simulate CPU spike (0-100%)
    cpu_usage = random.randint(50, 100)

    # Simulate memory usage
    memory_available = random.randint(1000, 16000)

    # Simulate HTTP errors (0-30%)
    http_5xx_count = random.randint(0, 30)

    # Simulate queue backlog
    queue_jobs_pending = random.randint(0, 200)

    metric_data = f"""
# HELP node_cpu_seconds_total Simulated CPU usage percentage
# TYPE node_cpu_seconds_total gauge
node_cpu_seconds_total{{mode="idle",instance="test-node"}} {100 - cpu_usage}

# HELP node_memory_MemAvailable_bytes Simulated available memory in bytes
# TYPE node_memory_MemAvailable_bytes gauge
node_memory_MemAvailable_bytes{{instance="test-node"}} {memory_available * 1024 * 1024}

# HELP http_requests_total Simulated 5xx HTTP request count
# TYPE http_requests_total counter
http_requests_total{{status="500",instance="test-app"}} {http_5xx_count}

# HELP queue_jobs_pending Simulated pending jobs in queue
# TYPE queue_jobs_pending gauge
queue_jobs_pending {queue_jobs_pending}
"""
    return Response(metric_data, mimetype="text/plain")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=9100)

üîπ How it works

CPU Usage ‚Üí random 50‚Äì100% ‚Üí triggers HighCPUUsageWarning or Critical.

Memory Available ‚Üí random 1‚Äì16 GB ‚Üí triggers HighMemoryUsageWarning/Critical.

HTTP 5xx Errors ‚Üí random 0‚Äì30 ‚Üí triggers HighErrorRateWarning/Critical.

Queue Backlog ‚Üí random 0‚Äì200 ‚Üí triggers QueueBacklog alert.

Prometheus can scrape this on http://<exporter-host>:9100/metrics.

üîπ Steps to run

Install dependencies:

pip install flask


Run exporter:

python mock_exporter.py


Update prometheus.yml scrape config:

scrape_configs:
  - job_name: 'mock-exporter'
    static_configs:
      - targets: ['localhost:9100']


Restart Prometheus and Alertmanager.

The metrics will randomly trigger your alerts ‚Äî check Status ‚Üí Rules in Prometheus and your Alertmanager channels (Slack, PagerDuty, Email).

‚úÖ Benefits:

Real Prometheus scrape ‚Üí more realistic than POSTing metrics.

Easy to extend: you can add more metrics for Kubernetes pods, latency, disk, etc.

You can simulate all severities: info, warning, critical.

----------------------------------------------------------------------

let‚Äôs make a ‚Äúforce alert‚Äù version of the Python exporter so you can immediately trigger critical, warning, and info alerts for end-to-end testing.

1Ô∏è‚É£ Python ‚ÄúForce Alert‚Äù Exporter (mock_exporter_force_alerts.py)
from flask import Flask, Response

app = Flask(__name__)

# Force metrics to trigger alerts
FORCE_CPU = 96             # CPU > 95% ‚Üí critical
FORCE_MEMORY = 9700        # Memory low ‚Üí critical
FORCE_5XX = 25             # 5xx rate ‚Üí critical
FORCE_QUEUE_BACKLOG = 150  # Queue backlog ‚Üí warning
FORCE_4XX_RATE = 50        # 4xx rate ‚Üí info

@app.route("/metrics")
def metrics():
    metric_data = f"""
# HELP node_cpu_seconds_total Simulated CPU usage percentage
# TYPE node_cpu_seconds_total gauge
node_cpu_seconds_total{{mode="idle",instance="test-node"}} {100 - FORCE_CPU}

# HELP node_memory_MemAvailable_bytes Simulated available memory in bytes
# TYPE node_memory_MemAvailable_bytes gauge
node_memory_MemAvailable_bytes{{instance="test-node"}} {FORCE_MEMORY * 1024 * 1024}

# HELP http_requests_total Simulated 5xx HTTP request count
# TYPE http_requests_total counter
http_requests_total{{status="500",instance="test-app"}} {FORCE_5XX}

# HELP queue_jobs_pending Simulated pending jobs in queue
# TYPE queue_jobs_pending gauge
queue_jobs_pending {FORCE_QUEUE_BACKLOG}

# HELP http_requests_total Simulated 4xx HTTP request count
# TYPE http_requests_total counter
http_requests_total{{status="400",instance="test-app"}} {FORCE_4XX_RATE}
"""
    return Response(metric_data, mimetype="text/plain")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=9100)

üîπ How It Works
Metric	Forced Value	Severity Triggered
CPU Usage (node_cpu_seconds_total)	96%	HighCPUUsageCritical (critical)
Memory (node_memory_MemAvailable_bytes)	9700 MB	HighMemoryUsageCritical (critical)
HTTP 5xx Errors (http_requests_total)	25	HighErrorRateCritical (critical)
Queue backlog (queue_jobs_pending)	150	QueueBacklog (warning)
HTTP 4xx Errors (http_requests_total)	50	High4xxRate (info)

All metrics are stable and will consistently trigger alerts.

You can adjust the values to test warning thresholds vs critical thresholds.

üîπ Steps to Use

Save as mock_exporter_force_alerts.py.

Run it:

python mock_exporter_force_alerts.py


Make sure Prometheus scrapes it (prometheus.yml):

scrape_configs:
  - job_name: 'mock-exporter'
    static_configs:
      - targets: ['localhost:9100']


Restart Prometheus.

Alerts should fire immediately, and you can verify routing:

Critical ‚Üí PagerDuty

Warning ‚Üí Slack

Info ‚Üí Email

Open Alertmanager UI (http://localhost:9093) to see alerts grouped by severity.

‚úÖ Now you have a full end-to-end test setup:

Prometheus rules ‚Üí prometheus-alert-rules.yml

Alertmanager routing ‚Üí prom-alertmanager.yml

Scrape metrics ‚Üí mock_exporter_force_alerts.py

Alerts fire ‚Üí routed to Slack / PagerDuty / Email based on severity

---------------------------------------------------------------------

Folder Structure 

prometheus-demo/
‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îî‚îÄ‚îÄ prometheus-alert-rules.yml
‚îú‚îÄ‚îÄ alertmanager/
‚îÇ   ‚îî‚îÄ‚îÄ prom-alertmanager.yml
‚îî‚îÄ‚îÄ mock_exporter/
    ‚îî‚îÄ‚îÄ mock_exporter_force_alerts.py



--------------------------------------------------------------------

1Ô∏è‚É£ Prometheus Config (prometheus/prometheus.yml)
global:
  scrape_interval: 20s
  evaluation_interval: 20s
  external_labels:
    monitor: 'prometheus-demo'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets: ["localhost:9093"]

# Scrape configs
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'mock-exporter'
    static_configs:
      - targets: ['localhost:9100']

# Reference alert rules
rule_files:
  - "prometheus-alert-rules.yml"

2Ô∏è‚É£ Prometheus Alert Rules (prometheus/prometheus-alert-rules.yml)

Use the severity-enhanced rules we created earlier:

groups:
  - name: infrastructure-alerts
    rules:
      - alert: HighCPUUsageCritical
        expr: 100 - (node_cpu_seconds_total{mode="idle",instance="test-node"}) > 95
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "High CPU usage CRITICAL on {{ $labels.instance }}"
          description: "CPU usage > 95% for 1m. Immediate action required!"

      - alert: HighMemoryUsageCritical
        expr: (1 - (node_memory_MemAvailable_bytes{instance="test-node"} / (16 * 1024 * 1024 * 1024))) * 100 > 95
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage CRITICAL on {{ $labels.instance }}"
          description: "Memory usage > 95% for 1m. Immediate action required!"

      - alert: HighErrorRateCritical
        expr: (http_requests_total{status="500",instance="test-app"}) / 100 > 0.2
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "High 5xx error rate CRITICAL on {{ $labels.instance }}"
          description: "5xx errors > 20% of total requests for 1m."

      - alert: QueueBacklog
        expr: queue_jobs_pending > 100
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Queue backlog too high"
          description: "Pending jobs in queue exceed 100 for 1m."

      - alert: High4xxRate
        expr: http_requests_total{status="400",instance="test-app"} > 20
        for: 1m
        labels:
          severity: info
        annotations:
          summary: "High 4xx rate on {{ $labels.instance }}"
          description: "More than 20 4xx requests in 1m."

3Ô∏è‚É£ Alertmanager Config (alertmanager/prom-alertmanager.yml)
global:
  resolve_timeout: 5m

route:
  receiver: 'default'
  group_by: ['alertname', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 1h

  routes:
    - match:
        severity: critical
      receiver: 'pagerduty-team'

    - match:
        severity: warning
      receiver: 'slack-team'

    - match:
        severity: info
      receiver: 'email-team'

receivers:
  - name: 'default'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXXXX/XXXXX/XXXXXXXX'
        channel: '#general-alerts'

  - name: 'pagerduty-team'
    pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_KEY'

  - name: 'slack-team'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXXXX/XXXXX/XXXXXXXX'
        channel: '#infra-warnings'

  - name: 'email-team'
    email_configs:
      - to: 'ops-team@example.com'

4Ô∏è‚É£ Mock Exporter (mock_exporter/mock_exporter_force_alerts.py)
from flask import Flask, Response

app = Flask(__name__)

# Force metrics to trigger alerts
FORCE_CPU = 96             # CPU > 95% ‚Üí critical
FORCE_MEMORY = 9700        # Memory low ‚Üí critical
FORCE_5XX = 25             # 5xx rate ‚Üí critical
FORCE_QUEUE_BACKLOG = 150  # Queue backlog ‚Üí warning
FORCE_4XX_RATE = 50        # 4xx rate ‚Üí info

@app.route("/metrics")
def metrics():
    metric_data = f"""
# HELP node_cpu_seconds_total Simulated CPU usage percentage
# TYPE node_cpu_seconds_total gauge
node_cpu_seconds_total{{mode="idle",instance="test-node"}} {100 - FORCE_CPU}

# HELP node_memory_MemAvailable_bytes Simulated available memory in bytes
# TYPE node_memory_MemAvailable_bytes gauge
node_memory_MemAvailable_bytes{{instance="test-node"}} {FORCE_MEMORY * 1024 * 1024}

# HELP http_requests_total Simulated 5xx HTTP request count
# TYPE http_requests_total counter
http_requests_total{{status="500",instance="test-app"}} {FORCE_5XX}

# HELP queue_jobs_pending Simulated pending jobs in queue
# TYPE queue_jobs_pending gauge
queue_jobs_pending {FORCE_QUEUE_BACKLOG}

# HELP http_requests_total Simulated 4xx HTTP request count
# TYPE http_requests_total counter
http_requests_total{{status="400",instance="test-app"}} {FORCE_4XX_RATE}
"""
    return Response(metric_data, mimetype="text/plain")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=9100)

5Ô∏è‚É£ How to Run the Demo

Install Python Flask:

pip install flask


Start Alertmanager:

alertmanager --config.file=alertmanager/prom-alertmanager.yml


Start Prometheus:

prometheus --config.file=prometheus/prometheus.yml


Start the mock exporter:

python mock_exporter/mock_exporter_force_alerts.py


Open UIs:

Prometheus ‚Üí http://localhost:9090 ‚Üí Status ‚Üí Rules

Alertmanager ‚Üí http://localhost:9093 ‚Üí see alerts grouped by severity

Verify notifications:

Critical ‚Üí PagerDuty

Warning ‚Üí Slack

Info ‚Üí Email

----------------------------------------------------------------------------
PURPOSE OF MOCK EXPORTER 

if you already have Node Exporter set up, you don‚Äôt need the mock exporter at all. ‚úÖ

Here‚Äôs the clarification:

1Ô∏è‚É£ Purpose of the Mock Exporter

It‚Äôs only needed if you want to simulate metrics (CPU spikes, memory low, 5xx errors) without touching real servers or apps.

It‚Äôs a testing tool for demo/testing environments.

2Ô∏è‚É£ Using Node Exporter Instead

Since you already have Node Exporter:

Prometheus will scrape real metrics from your hosts.

All infrastructure alerts (HighCPUUsage, HighMemoryUsage, DiskSpaceRunningLow, NodeDown) will fire based on actual metrics.

You don‚Äôt need to install or run anything extra for these alerts.

‚ö†Ô∏è Limitation:

You cannot test fake application alerts (like HighErrorRate or QueueBacklog) unless your application exposes Prometheus metrics for them.

3Ô∏è‚É£ Recommendation

Keep your current Node Exporter setup.

Use prometheus-alert-rules.yml to monitor real metrics.

If you want to test the full alert pipeline (including Slack/PagerDuty/Email) without impacting production, you can temporarily use a mock exporter or a small mock app exposing metrics.

Otherwise, just rely on real metrics and real alert triggers.

üí° Quick tip:
You can still test alert routing in Alertmanager by temporarily lowering thresholds in your rules (e.g., CPU > 10% instead of 80%) so Node Exporter metrics will immediately trigger alerts, without needing a mock exporter.